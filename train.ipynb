{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "!export MAKEFLAGS=\"-j$(nproc)\"\n",
    "!pip install numpy torch\n",
    "!pip install --upgrade huggingface_hub[hf_xet] hf_xet peft diffusers transformers accelerate xformers # flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import base64\n",
    "k = base64.b64decode('aGZfaHhua0Vaek5SaUtUVUFvRUFvcmJ3d0JTbHNmR2xsaWt5SQ==').decode()\n",
    "login(token=k, add_to_git_credential=False)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_XET_DOWNLOAD=1\n",
    "%env HF_XET_HIGH_PERFORMANCE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "conda create -n facefusion python=3.12 pip=25.0 -y\n",
    "conda init bash\n",
    "exec \"$SHELL\"\n",
    "conda activate facefusion\n",
    "\n",
    "git clone https://github.com/facefusion/facefusion\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"=== FaceFusion Fix Script ===\"\n",
    "echo \"Applying fixes to resolve circular import error and disable NSFW checks...\"\n",
    "\n",
    "sed -i '/def detect_nsfw/,/def detect_with_nsfw_1/{//!d}' facefusion/content_analyser.py \\\n",
    "  && sed -i '/def detect_nsfw/a\\       return False' facefusion/content_analyser.py \\\n",
    "# Set is_valid = True\n",
    "  && sed -i 's/^ *is_valid = .*/       is_valid = True/' facefusion/core.py\n",
    "\n",
    "# inside conda\n",
    "sudo apt update\n",
    "sudo apt install -y libcudnn9-cuda-12 libcudnn9-dev-cuda-12\n",
    "sudo ldconfig\n",
    "conda install -c nvidia cudnn=9.11 -y \n",
    "# Set runtime path once per shell\n",
    "export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python3 install.py --onnxruntime cuda     # or  --onnxruntime default  for CPU\n",
    "\n",
    "python3 facefusion.py run --open-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37863ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new  Python 3.11 virtual-env (optional but recommended)\n",
    "!python -m venv flux-lora && source flux-lora/bin/activate\n",
    "\n",
    "# !pip install -U diffusers transformers accelerate bitsandbytes safetensors datasets peft huggingface-hub wandb  # wandb optional, just for training charts\n",
    "\n",
    "!hf download black-forest-labs/FLUX.1-dev --local-dir flux_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# all this has to be paste in to the terminal + set up CF tunnel\n",
    "# pip install flash_attn-2.7.4.post1 --no-build-isolation --no-cache-dir\n",
    "pip install --upgrade pip\n",
    "pip install https://huggingface.co/spaces/Wauplin/gradio_logsview/resolve/main/gradio_logsview-0.0.5-py3-none-any.whl\n",
    "git clone https://github.com/cocktailpeanut/fluxgym\n",
    "cd fluxgym\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "pip install gradio slugify hf-transfer timm huggingface-hub torchvision wandb  --upgrade\n",
    "pip install -r requirements.txt\n",
    "git clone -b sd3 https://github.com/kohya-ss/sd-scripts\n",
    "cd sd-scripts\n",
    "pip install -r requirements.txt\n",
    "cd ..\n",
    "pip install huggingface-hub==0.25.2\n",
    "pip install triton bitsandbytes --upgrade\n",
    "python3 app.py\n",
    "\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feead5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user gdown\n",
    "!gdown --id 1y81XF2JyHMR0PgpcbEx2tiweWjbrljbE -O dt.tar && tar -xvf dt.tar && rm dt.tar # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b033811",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root/char\n",
    "pip install hyvideo --upgrade\n",
    "git clone https://github.com/tdrussell/diffusion-pipe\n",
    "cd diffusion-pipe\n",
    "pip install -r requirements.txt   # torch, xformers, deepspeed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# unsure about this config cell\n",
    "touch /root/char/diffusion-pipe/char_chroma_lora.toml\n",
    "mkdir -p /root/char/lora/\n",
    "\n",
    "cat << 'EOF' > /root/char/diffusion-pipe/char_chroma_lora.toml\n",
    "[model]\n",
    "type             = \"chroma\"\n",
    "transformer_path = \"/root/char/models/chroma/chroma-unlocked-v48.safetensors\"\n",
    "dtype            = \"bfloat16\"            # 4090 handles bf16 natively\n",
    "flux_shift       = true                  # critical for Chroma stability\n",
    "\n",
    "[adapter]\n",
    "type  = \"lora\"\n",
    "rank  = 64                               # good trade-off  (32–128 work)\n",
    "dtype = \"bfloat16\"\n",
    "\n",
    "[optimizer]\n",
    "type          = \"adamw\"\n",
    "lr            = 1e-4                     # Chroma blows up above 1e-3\n",
    "betas         = [0.9, 0.99]\n",
    "weight_decay  = 0.01\n",
    "eps           = 1e-8\n",
    "\n",
    "[train]\n",
    "resolution         = 1024\n",
    "max_steps          = 6000                # 150 × 40 images\n",
    "checkpoint_every   = 200\n",
    "masked_loss_ratio  = 0.1                 # 10 % bg masking keeps likeness sharp\n",
    "save_dir           = \"/root/char/lora/\"\n",
    "\n",
    "[data]\n",
    "root         = \"/root/char/dataset\"\n",
    "image_ext    = \"png\"\n",
    "caption_ext  = \"txt\"\n",
    "center_pad   = true\n",
    "center_pad_color = \"#777777\"\n",
    "\n",
    "[sample]\n",
    "# ——— WHEN  ———\n",
    "sample_every      = 200        # fire after every 200 training steps\n",
    "# ——— WHAT  ———\n",
    "prompts = [\n",
    "  \"lumifawn\",\n",
    "  \"lumifawn standing on a balcony wearing white crop top and white tight shorts\",\n",
    "  \"selfie of lumifawn on a tropical beach\"\n",
    "]\n",
    "negative_prompt   = \"low quality, bad anatomy, extra digits, missing digits, extra limbs, missing limbs, blur, bokeh\"\n",
    "num_inference_steps = 20\n",
    "guidance_scale      = 4.0\n",
    "width  = 768\n",
    "height = 768\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca05256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# GPT5 written script that does work\n",
    "\n",
    "# Chroma LoRA Training Setup Script\n",
    "# This script sets up the environment and fixes all issues to train Chroma LoRA\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"🚀 Setting up Chroma LoRA training environment...\"\n",
    "\n",
    "# 1. Install missing dependencies\n",
    "echo \"📦 Installing dependencies...\"\n",
    "pip install mpi4py\n",
    "pip install flash-attn==2.8.0.post2 xformers==0.0.31.post1\n",
    "pip install torchvision==0.22.0\n",
    "\n",
    "# 2. Initialize git submodules\n",
    "echo \"🔧 Initializing git submodules...\"\n",
    "cd /root/char/diffusion-pipe\n",
    "git submodule update --init --recursive\n",
    "\n",
    "# 3. Create dataset configuration\n",
    "echo \"📝 Creating dataset configuration...\"\n",
    "cat > dataset.toml << 'EOF'\n",
    "resolutions = [512]\n",
    "\n",
    "[[directory]]\n",
    "path = '/root/char/dataset'\n",
    "root = '/root/char/dataset'\n",
    "image_ext = 'png'\n",
    "caption_ext = 'txt'\n",
    "center_pad = true\n",
    "EOF\n",
    "\n",
    "# 4. Create main training configuration\n",
    "echo \"⚙️ Creating training configuration...\"\n",
    "cat > char_chroma_lora.toml << 'EOF'\n",
    "save_every_n_steps = 200\n",
    "dataset = 'dataset.toml'\n",
    "output_dir = '/root/char/char_chroma'\n",
    "epochs = 10\n",
    "micro_batch_size_per_gpu = 1\n",
    "gradient_accumulation_steps = 1\n",
    "activation_checkpointing = true\n",
    "reentrant_activation_checkpointing = true\n",
    "\n",
    "[model]\n",
    "type             = 'chroma'\n",
    "diffusers_path   = '/root/char/models/FLUX.1-dev'\n",
    "transformer_path = '/root/char/models/chroma-unlocked-v48-detail-calibrated.safetensors'\n",
    "dtype            = 'float16'\n",
    "transformer_dtype = 'float16'\n",
    "flux_shift       = true\n",
    "\n",
    "[adapter]\n",
    "type  = 'lora'\n",
    "rank  = 16\n",
    "dtype = 'float16'\n",
    "\n",
    "[optimizer]\n",
    "type          = 'adamw_optimi'\n",
    "lr            = 2e-4\n",
    "betas         = [0.9, 0.99]\n",
    "weight_decay  = 0.01\n",
    "eps           = 1e-8\n",
    "\n",
    "[train]\n",
    "# 28 images → ~150 steps per image is a good start\n",
    "max_steps          = 60000\n",
    "resolution         = 512\n",
    "masked_loss_ratio  = 0.10          # 10% background masking (matches the Reddit note)\n",
    "checkpoint_every   = 200\n",
    "save_dir           = '/root/char/char_chroma'\n",
    "bucket_reso        = true           # keep your mixed 1024^2 and 800x1200 set \"as is\"\n",
    "min_bucket_reso    = 512\n",
    "max_bucket_reso    = 512\n",
    "bucket_step        = 64\n",
    "# If you OOM on a 24 GB card, add:\n",
    "gradient_checkpointing = true\n",
    "\n",
    "[data]\n",
    "root         = '/root/char/dataset'     # folder containing 0.jpg…27.jpg and 0.txt…27.txt\n",
    "image_ext    = 'png'\n",
    "caption_ext  = 'txt'\n",
    "center_pad   = true                  # keep full 800x1200 frames; set to false to crop instead\n",
    "\n",
    "# Optional automatic samples every checkpoint (kept minimal)\n",
    "[sample]\n",
    "sample_every        = 200\n",
    "prompts             = [\n",
    "  \"lumifawn\",\n",
    "  \"lumifawn standing on a balcony wearing white crop top and white tight shorts\",\n",
    "  \"selfie of lumifawn on a tropical beach\"\n",
    "]\n",
    "negative_prompt     = \"extra limbs, text artifacts\"\n",
    "num_inference_steps = 25\n",
    "guidance_scale      = 4.0\n",
    "width               = 1024\n",
    "height              = 1024\n",
    "dtype               = 'fp16'\n",
    "EOF\n",
    "\n",
    "# 5. Modify Chroma model directly for memory optimization\n",
    "echo \"🔧 Modifying Chroma model for memory optimization...\"\n",
    "\n",
    "# Backup original file\n",
    "cp models/chroma.py models/chroma.py.backup\n",
    "\n",
    "# Use sed to modify the file directly\n",
    "sed -i '/self.diffusers_pipeline = diffusers.FluxPipeline.from_pretrained/a\\\n",
    "        # Move VAE and text encoders to CPU to save GPU memory\\\n",
    "        if hasattr(self.diffusers_pipeline, \"vae\"):\\\n",
    "            self.diffusers_pipeline.vae = self.diffusers_pipeline.vae.to(\"cpu\")\\\n",
    "        if hasattr(self.diffusers_pipeline, \"text_encoder\"):\\\n",
    "            self.diffusers_pipeline.text_encoder = self.diffusers_pipeline.text_encoder.to(\"cpu\")\\\n",
    "        if hasattr(self.diffusers_pipeline, \"text_encoder_2\"):\\\n",
    "            self.diffusers_pipeline.text_encoder_2 = self.diffusers_pipeline.text_encoder_2.to(\"cpu\")\\\n",
    "        if hasattr(self.diffusers_pipeline, \"tokenizer\"):\\\n",
    "            self.diffusers_pipeline.tokenizer = self.diffusers_pipeline.tokenizer\\\n",
    "        if hasattr(self.diffusers_pipeline, \"tokenizer_2\"):\\\n",
    "            self.diffusers_pipeline.tokenizer_2 = self.diffusers_pipeline.tokenizer_2' models/chroma.py\n",
    "\n",
    "# Add transformer to GPU after the precision conversion\n",
    "sed -i '/p.data = p.data.to(transformer_dtype)/a\\\n",
    "        # Ensure transformer is on GPU for LoRA training\\\n",
    "        self.transformer = self.diffusers_pipeline.transformer.to(\"cuda\")' models/chroma.py\n",
    "\n",
    "# 6. Create debug script for testing config\n",
    "echo \"🐛 Creating debug script...\"\n",
    "cat > debug_config.py << 'EOF'\n",
    "import toml\n",
    "import json\n",
    "\n",
    "with open('char_chroma_lora.toml') as f:\n",
    "    config = json.loads(json.dumps(toml.load(f)))\n",
    "\n",
    "print(\"Config keys:\", list(config.keys()))\n",
    "print(\"save_every_n_steps in config:\", 'save_every_n_steps' in config)\n",
    "print(\"save_every_n_epochs in config:\", 'save_every_n_epochs' in config)\n",
    "if 'save_every_n_steps' in config:\n",
    "    print(\"save_every_n_steps value:\", config['save_every_n_steps'])\n",
    "EOF\n",
    "\n",
    "# 7. Test the configuration\n",
    "echo \"🧪 Testing configuration...\"\n",
    "python3 debug_config.py\n",
    "\n",
    "# 8. Run the training\n",
    "echo \"🎯 Starting training...\"\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True accelerate launch train.py --config char_chroma_lora.toml\n",
    "\n",
    "echo \"✅ Setup complete! Training should now work with memory optimizations.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
